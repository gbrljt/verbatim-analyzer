{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3441fb9-f76c-4114-82e5-afef0b6dc029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import IProgress\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from nlp_id.lemmatizer import Lemmatizer \n",
    "from nlp_id.postag import PosTag\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "lemmatizer = Lemmatizer() \n",
    "postagger = PosTag()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ageng-anugrah/indobert-large-p2-finetuned-ner\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"ageng-anugrah/indobert-large-p2-finetuned-ner\")\n",
    "\n",
    "#from https://huggingface.co/ageng-anugrah/indobert-large-p2-finetuned-ner\n",
    "def predict(model, tokenizer, sentence):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(sentence.split(),\n",
    "                    is_split_into_words = True,\n",
    "                    return_offsets_mapping=True, \n",
    "                    return_tensors=\"pt\",\n",
    "                    padding='max_length', \n",
    "                    truncation=True, \n",
    "                    max_length=512)\n",
    "    model.to(device)\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [model.config.id2label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "    prediction = []\n",
    "    for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "        #only predictions on first word pieces are important\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            prediction.append(token_pred[1])\n",
    "        else:\n",
    "            continue\n",
    "    return zip(sentence.split(), prediction)\n",
    "\n",
    "data = pd.read_csv(r\"Training Data _ Raw.csv\", sep=';', index_col=False)\n",
    "data_l = pd.read_csv(r\"Training Data _ Lemmatized.csv\", sep=';', index_col=False)\n",
    "pos_db = [\"FW\", \"JJ\", \"NN\", \"VB\", \"NNP\", \"IN\", \"NEG\"]\n",
    "\n",
    "for x in range(len(data)):\n",
    "    l=\"\"\n",
    "    m=data.at[x,'VERBATIM']\n",
    "    u=pd.DataFrame(postagger.get_pos_tag(m), columns=['word','pos'])\n",
    "    for y in range(len(u)):\n",
    "        if u.at[y,'pos'] in pos_db:\n",
    "            l = \" \".join([l,u.at[y,'word']])\n",
    "    v=pd.DataFrame(predict(model,tokenizer,l), columns=['word','ner'])\n",
    "    l=\"\"\n",
    "    for y in range(len(v)):\n",
    "        if v.at[y,'ner']!=\"I-PLACE\" and v.at[y,'ner']!=\"B-PLACE\":\n",
    "            l = \" \".join([l,v.at[y,'word']])\n",
    "        elif v.at[y,'ner']==\"B-PLACE\":\n",
    "            l = \" \".join([l,\"loc\"])\n",
    "    l = lemmatizer.lemmatize(l)\n",
    "    data_l.at[x,'VERBATIM'] = l\n",
    "    if x%100==0:\n",
    "        print(x)\n",
    "        data_l.to_csv(r\"C:\\Users\\USER\\Desktop\\!!!\\Kategori Verbatim BSQ & CE - Training Data 21-23 _ Lemmatized.csv\", sep=';', na_rep=' ', index=False)\n",
    "data_l.to_csv(r\"C:\\Users\\USER\\Desktop\\!!!\\Kategori Verbatim BSQ & CE - Training Data 21-23 _ Lemmatized.csv\", sep=';', na_rep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eb2719-3893-4311-9c86-f75981946e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r\"Training Data _ Lemmatized.csv\", sep=';')\n",
    "X=df.iloc[:,1] #verbatim\n",
    "y=df.iloc[:,-2] #y=df.iloc[:,-3] \n",
    "d=y.unique().tolist()\n",
    "X=X.tolist()\n",
    "y=y.tolist()\n",
    "for L in range(0, len(y)):\n",
    "    y[L] = d.index(y[L])\n",
    "X=np.asarray(X)\n",
    "y=np.asarray(y)\n",
    "np.object = object\n",
    "np.unicode = str\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9)\n",
    "clf = ak.TextClassifier(overwrite=True, max_trials=4) #more than 4 would cause the 'text_block_1/max_tokens' error\n",
    "clf.fit(X_train, y_train, epochs=15, validation_split=0.1)\n",
    "print(clf.evaluate(X_test, y_test))\n",
    "m=clf.export_model()\n",
    "try:\n",
    "    m.save(\"model_div\", save_format=\"tf\") #m.save(\"model_ska\", save_format=\"tf\")\n",
    "except Exception:\n",
    "    m.save(\"model_div.h5\") #m.save(\"model_ska.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
