{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7021a-e9d2-4732-856f-1e8c6392c541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#SECTION 1 - IMPORTING PACKAGES\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "import autokeras as ak\n",
    "import calendar\n",
    "import datetime\n",
    "import locale\n",
    "import nlp_id\n",
    "import ntpath\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sklearn\n",
    "import tensorflow\n",
    "import webbrowser\n",
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "from nlp_id.postag import PosTag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#SECTION 2 - DECLARATIONS\n",
    "\n",
    "##Here I set the display settings of pandas' DataFrames. This is required so that in the finder function, the multiline will display the search result without truncation.\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "##Here I load the location database.\n",
    "location_data=pd.read_csv(r\"CONST/location_data.csv\", sep=None, decimal=\",\", index_col=False, engine='python', dtype={'Branch Code': str, 'KCU Code': str})\n",
    "location_data['Latitude'] = pd.to_numeric(location_data['Latitude'])\n",
    "location_data['Longitude'] = pd.to_numeric(location_data['Longitude'])\n",
    "##Here I declare settings to display month names.\n",
    "locale.setlocale(locale.LC_TIME, \"id_ID\")\n",
    "mos = calendar.month_name[1:]\n",
    "##Here I set up the POS tagger filter. Strictly do not modify the line directly below.\n",
    "pos_db = [\"FW\", \"JJ\", \"NN\", \"VB\", \"NNP\", \"IN\", \"NEG\"]\n",
    "##Here I load SKA & Divisi constants and map colors.\n",
    "with open(\"CONST/idxs.txt\") as f: \n",
    "    idxs = eval(f.read())\n",
    "with open(\"CONST/colors.txt\") as f: \n",
    "    cdm = eval(f.read())\n",
    "##Here I declare question numbers based on their category.\n",
    "bsq_l=[4,6,11]\n",
    "bsq_d=[8,13,15]\n",
    "bsq=bsq_l+bsq_d+[19,22]\n",
    "ce_be=[5]\n",
    "ce_br=[7]\n",
    "ce=ce_be+ce_br+[14]\n",
    "all=['all']\n",
    "##Here I load built-in nlp_id functions.\n",
    "lemmatizer = Lemmatizer() \n",
    "postagger = PosTag()\n",
    "\n",
    "#SECTION 3 - METHODS\n",
    "\n",
    "##Here I write a custom filter to POS tag and lemmatize sentences.\n",
    "def process(data):\n",
    "    for x in range(len(data)):\n",
    "        l=\"\"\n",
    "        m=data[x]\n",
    "        u=pd.DataFrame(postagger.get_pos_tag(m), columns=['word','pos'])\n",
    "        for y in range(len(u)):\n",
    "            if u.at[y,'pos'] in pos_db:\n",
    "                l = \" \".join([l,u.at[y,'word']])\n",
    "        l = lemmatizer.lemmatize(l)\n",
    "        data[x] = l\n",
    "    return (data)\n",
    "\n",
    "##Here, modified from https://www.tutorialspoint.com/find-the-most-similar-sentence-in-the-file-to-the-input-sentence-nlp,\n",
    "##   I build a Tf-idf vectorizer that uses my custom sentence processor, sorts based on score, and takes top n results.\n",
    "def get_most_similar_sentences(sta, data, novs):\n",
    "    temp = data.loc[:, 'Verbatim']\n",
    "    sentences = list(lemmatizer.lemmatize(temp[x]) for x in range(len(data)))\n",
    "    sta_p = lemmatizer.lemmatize(sta)\n",
    "\n",
    "    #these 3 lines are unchanged\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([sta_p] + sentences)\n",
    "    similarity_scores = (tfidf_matrix * tfidf_matrix.T).A[0][1:]\n",
    "\n",
    "    data['sim'] = similarity_scores\n",
    "    novs = novs if novs < len(sentences) else len(sentences)\n",
    "    most_similar_indices = np.argpartition(similarity_scores, -novs)[-novs:]\n",
    "    most_similar_sentences = data.loc[data.index.isin(most_similar_indices)].copy()\n",
    "    most_similar_sentences.sort_values(by='sim', ascending=False, inplace=True)\n",
    "    most_similar_sentences.drop('sim', axis=1, inplace=True)\n",
    "    return most_similar_sentences\n",
    "\n",
    "#SECTION 4 - GUI SETTINGS\n",
    "\n",
    "##Font Styles and Size\n",
    "tmf=(\"Arial\", 20)\n",
    "bmf=(\"Arial\", 15)\n",
    "\n",
    "##Buttons\n",
    "###Main Menu\n",
    "button_model=sg.Button(\"Categorize Data\", key='model', font=bmf)\n",
    "button_map=sg.Button(\"View Map\", key='map', font=bmf)\n",
    "button_finder=sg.Button(\"Verbatim Finder\", key='finder', font=bmf)\n",
    "###Model\n",
    "button_cat=sg.Button(\"Kategorisasi data berdasarkan verbatim\", key='cat')\n",
    "button_train=sg.Button(\"Gunakan data untuk melatih model\", key='tra', disabled=True)  #Temporarily disabled until autokeras fixes the KeyError: 'text_block_1/max_tokens' error\n",
    "button_backm=sg.Button(\"Kembali ke menu\", key='backm')\n",
    "###Map\n",
    "button_update=sg.Button(\"Generate map\", key='upd')\n",
    "button_backs=sg.Button(\"Kembali ke menu\", key='backs')\n",
    "###Finder\n",
    "button_selectall=sg.Button(\"Pilih semua\", key='sall')\n",
    "button_unselectall=sg.Button(\"Kosongkan semua\", key='uall')\n",
    "button_run=sg.Button(\"Run\", key='run')\n",
    "button_backf=sg.Button(\"Kembali ke menu\", key='backf')\n",
    "\n",
    "##Checkboxes\n",
    "###Model\n",
    "check_csv=sg.Check('CSV', key='c_csv')\n",
    "check_excel=sg.Check('Excel', key='c_xls')\n",
    "###Map\n",
    "check_kws=sg.Check('Kanwil', key='c_k', enable_events=True, disabled=True)\n",
    "check_years=sg.Check('Kurun waktu', key='c_y', enable_events=True, disabled=True)\n",
    "check_vbs=sg.Check('Jenis verbatim', key='c_v', enable_events=True, disabled=True)\n",
    "###Finder\n",
    "check_kanwils=[[sg.Check('1', key='1'), sg.Check('2', key='2'), sg.Check('3', key='3'), sg.Check('4', key='4'), sg.Check('5', key='5'), sg.Check('6', key='6')],[sg.Check('7', key='7'), sg.Check('8', key='8'), sg.Check('9', key='9'), sg.Check('10', key='10'), sg.Check('11', key='11'), sg.Check('12', key='12')]]\n",
    "check_savef=sg.Check('Simpan hasil pencarian', key='savef')\n",
    "\n",
    "##Combo Boxes\n",
    "###Map\n",
    "combo_kws=sg.Combo([i for i in range(1, 13)], key='kws', disabled=True, readonly=True, enable_events=True)\n",
    "combo_kcu=sg.Combo([], key='kcu', disabled=True, size=(30,1), readonly=True)\n",
    "combo_mons=sg.Combo(mos, key='months_start', disabled=True, readonly=True, default_value='')\n",
    "combo_mone=sg.Combo(mos, key='months_end', disabled=True, readonly=True, default_value='')\n",
    "combo_qts=sg.Combo([\"BSQ\", \"CE\", \"Semua\"], key='qts', disabled=True, readonly=True, default_value=\"Semua\")\n",
    "###Finder\n",
    "combo_qtf=sg.Combo([\"Semua Verbatim\", \"BSQ Layanan\", \"BSQ Digital\", \"CE Branch Experience\", \"CE Branch Relationship\"], default_value=\"Semua Verbatim\" ,key='qtf', readonly=True)\n",
    "\n",
    "##File Browser\n",
    "file_browserm=sg.FileBrowse(key='filebrowsem', file_types=((\"\",\"*.xls *.xlsx *.csv\"),)) #Model - suffix M\n",
    "file_browsers=sg.FileBrowse(key='filebrowses', file_types=((\"\",\"*.xls *.xlsx *.csv\"),)) #Map - suffix S\n",
    "file_browserf=sg.FileBrowse(key='filebrowsef', file_types=((\"\",\"*.xls *.xlsx *.csv\"),)) #Finder - suffix F\n",
    "\n",
    "##Input Fields\n",
    "###File Input\n",
    "input_filem=sg.Input(\"\", key='filem', disabled=True, enable_events=True) #Model\n",
    "input_files=sg.Input(\"\", key='files', disabled=True, enable_events=True) #Map\n",
    "input_filef=sg.Input(\"\", key='filef', disabled=True, enable_events=True) #Finder\n",
    "###Input Sentence (Finder)\n",
    "input_sentence=sg.Input(\"\", key='sentence')\n",
    "\n",
    "##Multiline Displays\n",
    "multi=sg.Multiline(key='results', visible=False, size=(150, 10)) #Finder\n",
    "\n",
    "##Spins\n",
    "###Map\n",
    "spin_yrss=sg.Spin([], initial_value='', key='yrs_start', disabled=True, readonly=True)\n",
    "spin_yrse=sg.Spin([], initial_value='', key='yrs_end', disabled=True, readonly=True)\n",
    "###Finder\n",
    "spin_amt=sg.Spin([1], initial_value=1, key='novs', readonly=True)\n",
    "\n",
    "##Static Text\n",
    "header=sg.Text(\"Verbatim Analyzer\", key='header', font=tmf)\n",
    "\n",
    "##Layouts\n",
    "###Main Menu Wrap\n",
    "all_els=[[button_model,button_map,button_finder]]\n",
    "###Main Menu Layout\n",
    "main_layout = [[sg.Column(all_els,element_justification='c')]]\n",
    "###Model Menu Layout\n",
    "model_layout = [\n",
    "    [sg.Text(\"Pilih file\"), input_filem, file_browserm],\n",
    "    [button_cat, sg.Text(\"Simpan sebagai\"), check_csv, check_excel],\n",
    "    [sg.Text(\"atau\"), button_train, sg.Push(), sg.Push(), button_backm]\n",
    "]\n",
    "###Map Menu Layout\n",
    "map_layout = [\n",
    "    [sg.Text(\"Pilih file\"), input_files, file_browsers],\n",
    "    [sg.Text(\"Filter dan kelompokkan data berdasarkan:\")],\n",
    "    [check_kws, combo_kws, sg.Text(\"KCU\"), combo_kcu],\n",
    "    [check_years, sg.Text(\"Dari\"), combo_mons, spin_yrss, sg.Text(\"Sampai\"), combo_mone, spin_yrse],\n",
    "    [check_vbs, combo_qts],\n",
    "    [button_update, sg.Push(), sg.Push(), button_backs]\n",
    "]\n",
    "###Finder Menu Layout\n",
    "finder_layout = [\n",
    "    [sg.Text(\"Pilih file\"), input_filef, file_browserf],\n",
    "    [sg.HorizontalSeparator()],\n",
    "    [sg.Text(\"Kriteria teks:\"), input_sentence],\n",
    "    [sg.HorizontalSeparator()],\n",
    "    [sg.Text(\"Kanwil area pencarian:\")],\n",
    "    [sg.Column(check_kanwils)],\n",
    "    [button_selectall, button_unselectall],\n",
    "    [sg.HorizontalSeparator()],\n",
    "    [sg.Text(\"Cari \"), spin_amt, sg.Text(\" verbatim paling relevan\")],\n",
    "    [sg.HorizontalSeparator()],\n",
    "    [sg.Text(\"Question type:\"), combo_qtf],\n",
    "    [sg.HorizontalSeparator()],\n",
    "    [button_run, check_savef, sg.Push(), sg.Push(), button_backf],\n",
    "    [multi],\n",
    "]\n",
    "###Main Layout\n",
    "layout = [[header],[sg.Column(main_layout, key='mainmenu', visible=True), sg.Column(model_layout, visible=False, key='modelmenu'), sg.Column(map_layout, visible=False, key='mapmenu'), sg.Column(finder_layout, visible=False, key='findermenu')]]\n",
    "##Window\n",
    "window = sg.Window(\"Verbatim Analyzer\", layout)\n",
    "\n",
    "#SECTION 5 - MAIN PROGRAM\n",
    "\n",
    "while True:\n",
    "    event, values = window.Read()\n",
    "    if event in (None, 'Exit'):\n",
    "        break\n",
    "##MAIN MENU FUNCTIONALITIES\n",
    "###Load and Initialize Data Categorizer\n",
    "    if event == \"model\":\n",
    "        window['header'].update(\"Verbatim Categorizer\")\n",
    "        window['mainmenu'].update(visible=False)\n",
    "        window['modelmenu'].update(visible=True)\n",
    "        window['filem'].update(value='')\n",
    "        window['c_csv'].update(False)\n",
    "        window['c_xls'].update(False)\n",
    "        fta=\"\"\n",
    "        data = pd.DataFrame()\n",
    "###Load and Initialize Map Generator\n",
    "    if event == \"map\":\n",
    "        window['header'].update(\"Map Generator\")\n",
    "        window['mainmenu'].update(visible=False)\n",
    "        window['mapmenu'].update(visible=True)\n",
    "        window['files'].update(value='')\n",
    "        window['c_k'].update(False, disabled=True)\n",
    "        window['c_v'].update(False, disabled=True)\n",
    "        window['c_y'].update(False, disabled=True)\n",
    "        window['kws'].update(value='',disabled=True)\n",
    "        window['kcu'].update(value='',disabled=True)\n",
    "        window['months_start'].update(value='',disabled=True)\n",
    "        window['months_end'].update(value='',disabled=True)\n",
    "        window['yrs_start'].update(value='',disabled=True)\n",
    "        window['yrs_end'].update(value='',disabled=True)\n",
    "        window['qts'].update(value='Semua',disabled=True)\n",
    "        fta=\"\"\n",
    "        data = pd.DataFrame()\n",
    "###Load and Initialize Verbatim Finder\n",
    "    if event == \"finder\":\n",
    "        window['header'].update(\"Verbatim Finder\")\n",
    "        window['mainmenu'].update(visible=False)\n",
    "        window['findermenu'].update(visible=True)\n",
    "        window['filef'].update(value='')\n",
    "        window['sentence'].update(value='')\n",
    "        for x in range (1, 13):\n",
    "            window[str(x)].update(False)\n",
    "        window['novs'].update(value=1)\n",
    "        window['qtf'].update(value='Semua Verbatim')\n",
    "        window['savef'].update(False)\n",
    "        window['results'].update(value='', visible=False)\n",
    "        fta=\"\"\n",
    "        data = pd.DataFrame()\n",
    "##DATA CATEGORIZER FUNCTIONALITIES\n",
    "###File Reader\n",
    "    if event == \"filem\":\n",
    "        fta=values['filem']\n",
    "###Categorize Data\n",
    "    if event == \"cat\":\n",
    "        if fta.lower().endswith('.csv'):    \n",
    "            try:\n",
    "                data = pd.read_csv(fta, engine='python', sep=None)\n",
    "                data.columns= data.columns.str.title()\n",
    "                vl = [col for col in data.columns if \"Verbatim\" in col][0]\n",
    "                data.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                X=data[\"Verbatim\"].values.tolist()\n",
    "                gate=True\n",
    "            except:\n",
    "                sg.Popup('Data tidak valid!')\n",
    "                data = pd.DataFrame()\n",
    "                fta=\"\"\n",
    "                gate=False\n",
    "            if gate:\n",
    "                X=process(X)\n",
    "                X=np.asarray(X)\n",
    "                np.object = object\n",
    "                np.unicode = str\n",
    "                shutil.copytree('TC_SKA', os.getcwd(), dirs_exist_ok = True)\n",
    "                m=load_model(\"model_ska\")\n",
    "                m=ak.TextClassifier(m)\n",
    "                data['Ska'] = m.predict(X, verbose=0)\n",
    "                for L in range(len(data)):\n",
    "                    data.at[L,'Ska'] = idxs['Ska'][int(data.at[L,'Ska'])]\n",
    "                shutil.copytree('TC_DIV', os.getcwd(), dirs_exist_ok = True)\n",
    "                n=load_model(\"model_div\")\n",
    "                n=ak.TextClassifier(n)\n",
    "                data['Divisi'] = n.predict(X, verbose=0)\n",
    "                data['Sub Divisi'] = 'x'\n",
    "                for L in range(len(data)):\n",
    "                    data.at[L,'Divisi'] = idxs['Divisi'][int(data.at[L,'Divisi'])]\n",
    "                    with open(\"CONST/subdiv.txt\") as f: \n",
    "                        subdiv_search = eval(f.read())\n",
    "                    with open(\"CONST/fb.txt\") as f: \n",
    "                        fb = eval(f.read())\n",
    "                    try:\n",
    "                        qn = data.at[L,'Question Type']\n",
    "                    except:\n",
    "                        qn = None\n",
    "                    if fb.get(qn) is not None and data.at[L,'Divisi'] == 'FB': \n",
    "                        data.at[L,'Sub Divisi'] = fb[qn]\n",
    "                    elif data.at[L,'Divisi'] in subdiv_search:\n",
    "                        search_list=list(subdiv_search[data.at[L,'Divisi']].keys())\n",
    "                        for Q in range(len(search_list)):\n",
    "                            if search_list[Q] in re.sub('\\W+','',data.at[L,\"Verbatim\"].lower()):\n",
    "                                data.at[L,'Sub Divisi'] = subdiv_search[data.at[L,'Divisi']][search_list[Q]]\n",
    "                fn=ntpath.basename(fta)\n",
    "                if values['c_csv'] == True:\n",
    "                    data.to_csv(fn+\" - Categorized.csv\", sep=';', index=False)\n",
    "                if values['c_xls'] == True:\n",
    "                    data.to_excel(fn+\" - Categorized.xlsx\", index=False)\n",
    "                try:\n",
    "                    shutil.rmtree(\"model_ska\")\n",
    "                    shutil.rmtree(\"model_div\")\n",
    "                    shutil.rmtree(\"text_classifier\")\n",
    "                except:\n",
    "                    pass\n",
    "                sg.popup(\"Data sudah selesai diproses.\")    \n",
    "        else:\n",
    "            ds=pd.ExcelFile(fta)\n",
    "            big_gate=False\n",
    "            for sn in range (len(ds.sheet_names)):\n",
    "                try:\n",
    "                    data=pd.read_excel(fta, sheet_name=ds.sheet_names[sn], engine='openpyxl')\n",
    "                    data.columns= data.columns.str.title()\n",
    "                    vl = [col for col in data.columns if \"Verbatim\" in col][0]\n",
    "                    data.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                    X=data[\"Verbatim\"].values.tolist()\n",
    "                    gate=True\n",
    "                    big_gate=True\n",
    "                except Exception as e:\n",
    "                    sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                    data = pd.DataFrame()\n",
    "                    gate=False\n",
    "                if gate:\n",
    "                    X=process(X)\n",
    "                    X=np.asarray(X)\n",
    "                    np.object = object\n",
    "                    np.unicode = str\n",
    "                    shutil.copytree('TC_SKA', os.getcwd(), dirs_exist_ok = True)\n",
    "                    m=load_model(\"model_ska\")\n",
    "                    m=ak.TextClassifier(m)\n",
    "                    data['Ska'] = m.predict(X, verbose=0)\n",
    "                    for L in range(len(data)):\n",
    "                        data.at[L,'Ska'] = idxs['Ska'][int(data.at[L,'Ska'])]\n",
    "                    shutil.copytree('TC_DIV', os.getcwd(), dirs_exist_ok = True)\n",
    "                    n=load_model(\"model_div\")\n",
    "                    n=ak.TextClassifier(n)\n",
    "                    data['Divisi'] = n.predict(X, verbose=0)\n",
    "                    data['Sub Divisi'] = 'x'\n",
    "                    for L in range(len(data)):\n",
    "                        data.at[L,'Divisi'] = idxs['Divisi'][int(data.at[L,'Divisi'])]\n",
    "                        with open(\"CONST/subdiv.txt\") as f: \n",
    "                            subdiv_search = eval(f.read())\n",
    "                        with open(\"CONST/fb.txt\") as f: \n",
    "                            fb = eval(f.read())\n",
    "                        try:\n",
    "                            qn = data.at[L,'Question Type']\n",
    "                        except:\n",
    "                            qn = None\n",
    "                        if fb.get(qn) is not None and data.at[L,'Divisi'] == 'FB':\n",
    "                            data.at[L,'Sub Divisi'] = fb[qn]\n",
    "                        elif data.at[L,'Divisi'] in subdiv_search:\n",
    "                            search_list=list(subdiv_search[data.at[L,'Divisi']].keys())\n",
    "                            for Q in range(len(search_list)):\n",
    "                                if search_list[Q] in re.sub('\\W+','',data.at[L,\"Verbatim\"].lower()):\n",
    "                                    data.at[L,'Sub Divisi'] = subdiv_search[data.at[L,'Divisi']][search_list[Q]]\n",
    "                    fn=ntpath.basename(fta)\n",
    "                    if values['c_csv'] == True:\n",
    "                        data.to_csv(fn+\" \"+ds.sheet_names[sn]+\" - Categorized.csv\", sep=';', index=False)\n",
    "                    if values['c_xls'] == True:\n",
    "                        try:\n",
    "                            with pd.ExcelWriter(fn+\" - Categorized.xlsx\", mode=\"a\") as writer:\n",
    "                                data.to_excel(writer, engine='xlsxwriter', index=False, sheet_name=ds.sheet_names[sn])\n",
    "                        except:\n",
    "                            with pd.ExcelWriter(fn+\" - Categorized.xlsx\", mode=\"w\") as writer:\n",
    "                                data.to_excel(writer, engine='xlsxwriter', index=False, sheet_name=ds.sheet_names[sn])\n",
    "            if not big_gate:\n",
    "                sg.Popup('Data tidak valid!')\n",
    "                fta=\"\"\n",
    "            else:\n",
    "                try:\n",
    "                    shutil.rmtree(\"model_ska\")\n",
    "                    shutil.rmtree(\"model_div\")\n",
    "                    shutil.rmtree(\"text_classifier\")\n",
    "                except:\n",
    "                    pass\n",
    "                sg.popup(\"Data sudah selesai diproses.\")\n",
    "###Retrain Model - Currently Disabled Due to AutoKeras Bug\n",
    "    if event == \"tra\":\n",
    "        if fta.lower().endswith('.csv'): \n",
    "            try:\n",
    "                data = pd.read_csv(fta, engine='python', sep=None)\n",
    "                vl = [col for col in data.columns if \"Verbatim\" in col][0]\n",
    "                data.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                data = data[[\"Verbatim\", \"Ska\", \"Divisi\"]]\n",
    "            except:\n",
    "                data = pd.DataFrame()\n",
    "        else:\n",
    "            data = pd.DataFrame()\n",
    "            ds = pd.ExcelFile(fta)\n",
    "            for sn in range (len(ds.sheet_names)):\n",
    "                try:\n",
    "                    dr=pd.read_excel(fta, sheet_name=ds.sheet_names[sn], engine='openpyxl')\n",
    "                    dr.columns = dr.columns.str.title()\n",
    "                    vl = [col for col in dr.columns if \"Verbatim\" in col][0]\n",
    "                    dr.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                    if not data.empty:\n",
    "                        try:\n",
    "                            data = pd.concat([data,dr[[\"Verbatim\", \"Ska\", \"Divisi\"]]],ignore_index=True)\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                    else:\n",
    "                        try:\n",
    "                            data = dr[[\"Verbatim\", \"Ska\", \"Divisi\"]]\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                except Exception as e:\n",
    "                    sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "        if data.empty:\n",
    "            sg.Popup('Data tidak valid!')\n",
    "            fta=\"\"\n",
    "        else:    \n",
    "            X=data[\"Verbatim\"]\n",
    "            y=data[\"Ska\"]\n",
    "            w=data[\"Divisi\"]\n",
    "            X=X.tolist()\n",
    "            y=y.tolist()\n",
    "            w=w.tolist()\n",
    "            for L in range(0, len(y)):\n",
    "                try:\n",
    "                    y[L] = idxs[\"Ska\"].index(y[L])\n",
    "                except:\n",
    "                    idxs[\"Ska\"].append(y[L])\n",
    "                    y[L] = idxs[\"Ska\"].index(y[L])\n",
    "                    with open('CONST/idxs.txt','w') as data:  \n",
    "                        data.write(str(idxs))\n",
    "            for L in range(0, len(w)):\n",
    "                try:\n",
    "                    w[L] = idxs[\"Divisi\"].index(w[L])\n",
    "                except:\n",
    "                    idxs[\"Divisi\"].append(y[L])\n",
    "                    w[L] = idxs[\"Divisi\"].index(w[L])\n",
    "                    with open('CONST/idxs.txt','w') as data:  \n",
    "                        data.write(str(idxs))\n",
    "            X=np.asarray(X)\n",
    "            y=np.asarray(y)\n",
    "            w=np.asarray(w)\n",
    "            X=X.reshape(-1,1)\n",
    "            y=y.reshape(-1,1)\n",
    "            w=w.reshape(-1,1)\n",
    "            np.object = object\n",
    "            np.unicode = str\n",
    "            shutil.copytree('TC_SKA', os.getcwd(), dirs_exist_ok = True)\n",
    "            m=load_model(\"model_ska\")\n",
    "            m=ak.TextClassifier(m)\n",
    "            m.fit(X, y, epochs=15, validation_split=0.1, verbose=0)\n",
    "            try:\n",
    "                m.save(\"TC_SKA/model_ska\", save_format=\"tf\")\n",
    "            except Exception:\n",
    "                m.save(\"TC_SKA/model_ska.h5\")\n",
    "            shutil.copytree('TC_DIV', os.getcwd(), dirs_exist_ok = True)\n",
    "            n=load_model(\"model_div\")\n",
    "            n=ak.TextClassifier(n)\n",
    "            n.fit(X, w, epochs=15, validation_split=0.1, verbose=0)\n",
    "            try:\n",
    "                m.save(\"TC_DIV/model_div\", save_format=\"tf\")\n",
    "            except Exception:\n",
    "                m.save(\"TC_DIV/model_div.h5\")\n",
    "            shutil.rmtree(\"model_ska\")\n",
    "            shutil.rmtree(\"model_div\")\n",
    "            shutil.rmtree(\"text_classifier\")\n",
    "            sg.popup(\"Data sudah selesai diproses.\")\n",
    "    if event == \"backm\": #back to menu from model\n",
    "        window['header'].update(\"Verbatim Analyzer\")\n",
    "        window['mainmenu'].update(visible=True)\n",
    "        window['modelmenu'].update(visible=False)\n",
    "##MAP GENERATOR FUNCTIONALITIES\n",
    "###File Reader\n",
    "    if event == \"files\":\n",
    "        data = pd.DataFrame()\n",
    "        fta=values['files']\n",
    "        if fta.lower().endswith('.csv'):\n",
    "            try:\n",
    "                dr = pd.read_csv(fta, engine='python', sep=None, dtype={'Branch Code': str}, parse_dates=[\"Periode\"], dayfirst=True)\n",
    "                dr.columns = dr.columns.str.title()\n",
    "                vl = [col for col in dr.columns if \"Verbatim\" in col][0]\n",
    "                dr.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                kl = [col for col in dr.columns if \"Kanwil\" in col][0]\n",
    "                dr.rename(columns = {kl:'Kanwil'}, inplace = True)\n",
    "                data = dr[[\"Periode\", \"Kanwil\", \"Branch Code\", \"Question Type\", \"Verbatim\", \"Ska\"]]\n",
    "                gate = True\n",
    "            except Exception as e:\n",
    "                sg.Popup('Data tidak valid: '+str(e))\n",
    "                data = pd.DataFrame()\n",
    "                fta=\"\"\n",
    "                gate = False\n",
    "        else:\n",
    "            ds = pd.ExcelFile(fta)\n",
    "            for sn in range (len(ds.sheet_names)):\n",
    "                try:\n",
    "                    dr=pd.read_excel(fta, sheet_name=ds.sheet_names[sn], engine='openpyxl', dtype={'Branch Code': str}, parse_dates=[\"Periode\"])\n",
    "                    dr.columns = dr.columns.str.title()\n",
    "                    vl = [col for col in dr.columns if \"Verbatim\" in col][0]\n",
    "                    dr.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                    kl = [col for col in dr.columns if \"Kanwil\" in col][0]\n",
    "                    dr.rename(columns = {kl:'Kanwil'}, inplace = True)\n",
    "                    if not data.empty:\n",
    "                        try:\n",
    "                            data = pd.concat([data,dr[[\"Periode\", \"Kanwil\", \"Branch Code\", \"Question Type\", \"Verbatim\", \"Ska\"]]],ignore_index=True)\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                    else:\n",
    "                        try:\n",
    "                            data = dr[[\"Periode\", \"Kanwil\", \"Branch Code\", \"Question Type\", \"Verbatim\", \"Ska\"]]\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                except Exception as e:\n",
    "                    sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "            if not data.empty:\n",
    "                gate = True\n",
    "            else:\n",
    "                sg.Popup('Data tidak valid!')\n",
    "                fta=\"\"\n",
    "                gate = False\n",
    "        if gate:\n",
    "            data=data.replace('x', '')\n",
    "            data[\"year\"]=data[\"Periode\"].dt.year\n",
    "            for L in range(len(data)):\n",
    "                data.at[L,\"month\"]=calendar.month_name[data.at[L,\"Periode\"].month]\n",
    "            window['c_k'].update(disabled=False)\n",
    "            window['c_y'].update(disabled=False)\n",
    "            window['c_v'].update(disabled=False)\n",
    "        else:\n",
    "            window['kws'].update(value='',disabled=True)\n",
    "            window['kcu'].update(value='',disabled=True)\n",
    "            window['months_start'].update(value='',disabled=True)\n",
    "            window['months_end'].update(value='',disabled=True)\n",
    "            window['yrs_start'].update(value='',disabled=True)\n",
    "            window['yrs_end'].update(value='',disabled=True)\n",
    "            window['qts'].update(value='Semua',disabled=True)\n",
    "            window['c_k'].update(False, disabled=True)\n",
    "            window['c_y'].update(False, disabled=True)\n",
    "            window['c_v'].update(False, disabled=True)\n",
    "###Select Kanwil/KCU\n",
    "    if event == \"c_k\":\n",
    "        if values['c_k'] == True: #if enabled, set data\n",
    "            window['kws'].update(value=1, disabled=False)\n",
    "            kk=location_data[(location_data[\"Branch Code\"] == location_data[\"KCU Code\"]) & (location_data[\"Kanwil\"] == 1)][\"Branch Name\"].values.tolist()\n",
    "            window['kcu'].update(values=kk, disabled=False)\n",
    "        else: #if disabled, clear\n",
    "            window['kws'].update(value='', disabled=True)\n",
    "            window['kcu'].update(value='', disabled=True)\n",
    "            kk=[]\n",
    "###If Kanwil Selection Changed, Update KCU List\n",
    "    if event == \"kws\":\n",
    "        kk=location_data[(location_data[\"Branch Code\"] == location_data[\"KCU Code\"]) & (location_data[\"Kanwil\"] == int(values[\"kws\"]))][\"Branch Name\"].values.tolist()\n",
    "        window['kcu'].update(values=kk, disabled=False)\n",
    "###Select Timeframe\n",
    "    if event == \"c_y\":\n",
    "        if values['c_y'] == True: #if enabled, set data\n",
    "            window['months_start'].update(value=mos[min(data[\"Periode\"]).month-1], disabled=False)\n",
    "            window['months_end'].update(value=mos[max(data[\"Periode\"]).month-1], disabled=False)\n",
    "            window['yrs_start'].update(value=min(data[\"Periode\"]).year, values=list(range((min(data[\"Periode\"]).year), (max(data[\"Periode\"]).year)+1)), disabled=False)\n",
    "            window['yrs_end'].update(value=max(data[\"Periode\"]).year, values=list(range((min(data[\"Periode\"]).year), (max(data[\"Periode\"]).year)+1)), disabled=False)\n",
    "        else: #if disabled, clear\n",
    "            window['months_start'].update(value='',disabled=True)\n",
    "            window['months_end'].update(value='',disabled=True)\n",
    "            window['yrs_start'].update(value='',disabled=True)\n",
    "            window['yrs_end'].update(value='',disabled=True)\n",
    "###Select Question Type\n",
    "    if event == \"c_v\":\n",
    "        if values['c_v'] == True:\n",
    "            window['qts'].update(disabled=False)\n",
    "        else:\n",
    "            window['qts'].update(value='Semua', disabled=True)\n",
    "###Generate Map\n",
    "    if event == \"upd\":\n",
    "        fn=ntpath.basename(fta)\n",
    "        if data.empty: #error handling\n",
    "            sg.Popup('Data tidak valid!')\n",
    "        else:\n",
    "            data_proj = data.drop(data.index[(data['Ska'] == '')])\n",
    "            if values['c_y'] == True: #timeframe filter\n",
    "                date_from = pd.Timestamp(datetime.date(values['yrs_start'], mos.index(values['months_start'])+1, 1))\n",
    "                date_to = pd.Timestamp(datetime.date(values['yrs_end'], mos.index(values['months_end'])+1, 1))\n",
    "                if date_from>date_to:\n",
    "                    date_from = pd.Timestamp(datetime.date(values['yrs_start'], mos.index(values['months_start'])+1, calendar.monthrange(values['yrs_start'], mos.index(values['months_start'])+1)[1]))\n",
    "                    kd = (data_proj[\"Periode\"] >= date_to) & (data_proj[\"Periode\"] <= date_from)\n",
    "                    fn = fn + \" Periode \" + values['months_end'] + \" \" + str(values['yrs_end']) + \" - \" + values['months_start'] + \" \" + str(values['yrs_start'])\n",
    "                else:\n",
    "                    date_to = pd.Timestamp(datetime.date(values['yrs_end'], mos.index(values['months_end'])+1, calendar.monthrange(values['yrs_end'], mos.index(values['months_end'])+1)[1]))\n",
    "                    kd = (data_proj[\"Periode\"] >= date_from) & (data_proj[\"Periode\"] <= date_to)\n",
    "                    fn = fn + \" Periode \" + values['months_start'] + \" \" + str(values['yrs_start']) + \" - \" + values['months_end'] + \" \" + str(values['yrs_end'])\n",
    "                data_proj=data_proj.loc[kd]\n",
    "            if values['c_k'] == True: #kanwil filter\n",
    "                if values['kcu'] != '':\n",
    "                    kcuc=location_data[location_data[\"KCU Code\"] == location_data.loc[location_data[\"Branch Name\"]==values['kcu'],\"Branch Code\"].values[0]][\"Branch Code\"].values.tolist()\n",
    "                    data_proj = data_proj.loc[data_proj['Branch Code'].isin(kcuc)]\n",
    "                    fn = fn + \" KCU \" + values['kcu']\n",
    "                elif values['kws'] != '':\n",
    "                    data_proj = data_proj.loc[data_proj[[col for col in data.columns if \"Kanwil\" in col][0]]==int(values['kws'])]\n",
    "                    fn = fn + \" Kanwil \" + str(values['kws'])\n",
    "            if values['c_v'] == True: #question type filter\n",
    "                if values['qts']==\"BSQ\":\n",
    "                    data_proj['Question Type'] = data_proj['Question Type'].str.replace('\\D', '', regex=True)\n",
    "                    data_proj['Question Type'] = pd.to_numeric(data_proj['Question Type'])\n",
    "                    data_proj = data_proj[data_proj['Question Type'].isin(bsq)]\n",
    "                    fn = fn + \" - BSQ\"\n",
    "                elif values['qts']==\"CE\":\n",
    "                    data_proj['Question Type'] = data_proj['Question Type'].str.replace('\\D', '', regex=True)\n",
    "                    data_proj['Question Type'] = pd.to_numeric(data_proj['Question Type'])\n",
    "                    data_proj = data_proj[data_proj['Question Type'].isin(ce)]\n",
    "                    fn = fn + \" - CE\"\n",
    "                else:\n",
    "                    fn = fn + \" - BSQ & CE\"\n",
    "            else:\n",
    "                fn = fn + \" - BSQ & CE\"\n",
    "\n",
    "            if not (data_proj.empty):\n",
    "                location_data_proj = location_data.copy()\n",
    "                location_data_proj = location_data_proj.assign(Count=location_data_proj['Branch Code'].map(data_proj['Branch Code'].value_counts()))\n",
    "                location_data_proj[\"SKA_M\"]=\"\"\n",
    "                location_data_proj[\"SKA_M_C\"]=\"\"\n",
    "                location_data_proj[\"SKA_M_2\"]=\"\"\n",
    "                location_data_proj[\"SKA_M_C_2\"]=\"\"\n",
    "                location_data_proj.dropna(subset=['Count'], inplace=True)\n",
    "                data_proj = data_proj.reset_index(drop=True)\n",
    "                location_data_proj = location_data_proj.reset_index(drop=True)\n",
    "                for L in range(len(location_data_proj)):\n",
    "                    try:\n",
    "                        location_data_proj.at[L,\"SKA_M\"] = data_proj.loc[data_proj[\"Branch Code\"]==location_data_proj.at[L,'Branch Code'],'Ska'].mode()[0]\n",
    "                        location_data_proj.at[L,\"SKA_M_C\"] = len(data_proj.loc[(data_proj[\"Branch Code\"]==location_data_proj.at[L,'Branch Code']) & (data_proj[\"Ska\"]==location_data_proj.at[L,\"SKA_M\"])])\n",
    "                    except:\n",
    "                        location_data_proj.at[L,\"SKA_M\"] = \"Tidak Ada\"\n",
    "                        location_data_proj.at[L,\"SKA_M_C\"] = 0\n",
    "                    try:\n",
    "                        location_data_proj.at[L,\"SKA_M_2\"] = data_proj.loc[data_proj[\"Branch Code\"]==location_data_proj.at[L,'Branch Code'],'Ska'].value_counts().index[1]\n",
    "                        location_data_proj.at[L,\"SKA_M_C_2\"] = len(data_proj.loc[(data_proj[\"Branch Code\"]==location_data_proj.at[L,'Branch Code']) & (data_proj[\"Ska\"]==location_data_proj.at[L,\"SKA_M_2\"])])\n",
    "                    except:\n",
    "                        location_data_proj.at[L,\"SKA_M_2\"] = \"Tidak Ada\"\n",
    "                        location_data_proj.at[L,\"SKA_M_C_2\"] = 0\n",
    "                location_data_proj[\"SKA_M_C\"] = location_data_proj[\"SKA_M_C\"].astype(float)\n",
    "                location_data_proj[\"SKA_M_C_2\"] = location_data_proj[\"SKA_M_C_2\"].astype(float)\n",
    "                location_data_proj = location_data_proj.reset_index(drop=True)\n",
    "                vis = px.scatter_mapbox(location_data_proj, lat=\"Latitude\", lon=\"Longitude\", size=\"SKA_M_C\", title=\"Map - \" + fn, color='SKA_M', mapbox_style=\"carto-positron\", color_discrete_map=cdm, custom_data=['Count', 'SKA_M', 'SKA_M_C', 'SKA_M_2', 'SKA_M_C_2', 'Branch Name'])\n",
    "                vis.update_traces(hovertemplate=\"<b>%{customdata[5]}</b><br><br><b>Verbatim Terbanyak: %{customdata[1]}</b><br>dengan jumlah <b>%{customdata[2]} dari total %{customdata[0]} verbatim.</b><br>Verbatim kedua terbanyak: %{customdata[3]} (%{customdata[4]})<extra></extra>\")\n",
    "                vis.update_layout(autosize = True, legend_title=\"SKA\")\n",
    "                vis.write_html(f\"Map - {fn}.html\")\n",
    "                webbrowser.open_new_tab(f\"Map - {fn}.html\")\n",
    "                sg.popup(\"Data sudah selesai diproses.\")\n",
    "            else:\n",
    "                sg.Popup('Tidak ada data yang sesuai filter!')\n",
    "    if event == \"backs\": #back to main menu from map\n",
    "        window['header'].update(\"Verbatim Analyzer\")\n",
    "        window['mainmenu'].update(visible=True)\n",
    "        window['mapmenu'].update(visible=False)\n",
    "##FINDER FUNCTIONALITIES\n",
    "###File Reader\n",
    "    if event == \"filef\":\n",
    "        data = pd.DataFrame()\n",
    "        fta=values['filef']\n",
    "        if fta.lower().endswith('.csv'):\n",
    "            try:\n",
    "                dr = pd.read_csv(fta, sep=None, engine='python')\n",
    "                dr.columns = dr.columns.str.title()\n",
    "                vl = [col for col in dr.columns if \"Verbatim\" in col][0]\n",
    "                dr.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                kl = [col for col in dr.columns if \"Kanwil\" in col][0]\n",
    "                dr.rename(columns = {kl:'Kanwil'}, inplace = True)\n",
    "                data = dr[[\"Kanwil\", \"Question Type\", \"Verbatim\"]]\n",
    "                window['novs'].update(values=[i for i in range(1, len(data))])  #update number of verbatims spin\n",
    "            except Exception as e:\n",
    "                sg.Popup('Data tidak valid: '+str(e))\n",
    "                window['novs'].update(values=[1])\n",
    "                fta=\"\"\n",
    "        else:\n",
    "            ds = pd.ExcelFile(fta)\n",
    "            for sn in range (len(ds.sheet_names)):\n",
    "                try:\n",
    "                    dr=pd.read_excel(fta, sheet_name=ds.sheet_names[sn], engine='openpyxl')\n",
    "                    dr.columns = dr.columns.str.title() #rename columns\n",
    "                    vl = [col for col in dr.columns if \"Verbatim\" in col][0]\n",
    "                    dr.rename(columns = {vl:'Verbatim'}, inplace = True)\n",
    "                    kl = [col for col in dr.columns if \"Kanwil\" in col][0]\n",
    "                    dr.rename(columns = {kl:'Kanwil'}, inplace = True)\n",
    "                    if not data.empty:\n",
    "                        try:\n",
    "                            data = pd.concat([data,dr[[\"Kanwil\", \"Question Type\", \"Verbatim\"]]],ignore_index=True)\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                    else:\n",
    "                        try:\n",
    "                            data = dr[[\"Kanwil\", \"Question Type\", \"Verbatim\"]]\n",
    "                        except Exception as e:\n",
    "                            sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "                except Exception as e:\n",
    "                    sg.Popup('Sheet '+ds.sheet_names[sn]+' tidak valid: '+str(e))\n",
    "            if data.empty:\n",
    "                sg.Popup('Data tidak valid!')\n",
    "                window['novs'].update(values=[1])\n",
    "                fta=\"\"\n",
    "            else:\n",
    "                window['novs'].update(values=[i for i in range(1, len(data))]) #update number of verbatims spin\n",
    "###Select All Kanwils\n",
    "    if event == \"sall\":\n",
    "        for x in range (1, 13):\n",
    "            window[str(x)].update(True)\n",
    "###Unselect All Kanwils\n",
    "    if event == \"uall\":\n",
    "        for x in range (1, 13):\n",
    "            window[str(x)].update(False)\n",
    "###Run Search\n",
    "    if event == \"run\":\n",
    "\t#get data from gui\n",
    "        s=values['sentence']\n",
    "        n=values['novs']\n",
    "        c=values['qtf']\n",
    "        q=all\n",
    "\t#change q accordingly\n",
    "        match c:\n",
    "            case \"BSQ Layanan\":\n",
    "                q=bsq_l\n",
    "            case \"BSQ Digital\":\n",
    "                q=bsq_d\n",
    "            case \"CE Branch Experience\":\n",
    "                q=ce_be\n",
    "            case \"CE Branch Relationship\":\n",
    "                q=ce_br\n",
    "\t#select list of kanwils\n",
    "        search=[]\n",
    "        for x in range (1, 13):\n",
    "            if values[str(x)] == True:\n",
    "                search.append(x)\n",
    "\t#error handling\n",
    "        if data.empty:\n",
    "            sg.Popup('Tidak ada data yang sesuai kriteria!')\n",
    "        elif search==[]:\n",
    "            sg.Popup('Pilih minimal satu kanwil!')\n",
    "        elif not s:\n",
    "            sg.Popup('Masukkan teks untuk dicari!')\n",
    "        else:\n",
    "\t    #filter out kanwil selection\n",
    "            data[\"Kanwil\"] = pd.to_numeric(data[\"Kanwil\"], errors='coerce')\n",
    "            data_proj = data.loc[data[\"Kanwil\"].isin(search)].copy()\n",
    "            if q != all: #convert q to numbers\n",
    "                data_proj['Question Type'] = data_proj['Question Type'].str.replace('\\D', '', regex=True)\n",
    "                data_proj['Question Type'] = pd.to_numeric(data_proj['Question Type'])\n",
    "                data_proj = data_proj[data_proj['Question Type'].isin(q)]\n",
    "            data_proj = data_proj.reset_index(drop=True)\n",
    "            t=get_most_similar_sentences(s,data_proj,n)\n",
    "            window['results'].update(value=t, visible=True)\n",
    "            if values['savef'] == True: #save search results if checked\n",
    "                kws_l=','.join(map(str,search))\n",
    "                fn=\"Verbatim Search '\" + s + \"' pada \" + ntpath.basename(fta) + \" KW \" + kws_l + \" - \" + c + \" (Top \" + str(n) + \")\"\n",
    "                t.to_csv(r'{}.csv'.format(fn), sep=\";\", index=False)\n",
    "    if event == \"backf\": #back to menu from finder\n",
    "        window['header'].update(\"Verbatim Analyzer\")\n",
    "        window['mainmenu'].update(visible=True)\n",
    "        window['findermenu'].update(visible=False)\n",
    "        window['results'].update(value='')\n",
    "window.Close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
